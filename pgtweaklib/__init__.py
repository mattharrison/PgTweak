#!/usr/bin/env python
"""
PgTweak
========

A tool to run queries against different postgres settings.
Normally one would scour the postgres logs for slow queries.

Scouring logs for queries
--------------------------

Run the following to pull queries from log files::

  pgtweak  --extract-queries --log-file some/pg/log/file > extracted.sql

Getting a query file
---------------------

One can go through extracted.sql to find a query to example, or hand
write them in a text file.

Generating a sample config file
--------------------------------

A config file is used to indicate what settings to tweak.  It is a
json file that looks like this::

  [
    [
      "work_mem", 
      [
        "1MB", 
        "512MB"
      ]
    ], 
    [
      "maintenance_work_mem", 
      [
        "1MB", 
        "512MB"
      ]
    ]
  ]

That is the one generated by ::

  pgtweak --gen-config

You can add more settings and possible values for settings.  Remember
though, that it will test the combination of each setting.  For every
new value option you add to a setting, you double the runs.  (Note
that just adding an setting with a single value just includes that in
every run.)


testing settings
-----------------

Run::

  pgtweak --query-file simple.sql --analyze-queries --dburl postgres://postgres@localhost/testetl --config-file conf.json > results.json

Note that ``dburl`` is a SqlAlchemy style connection string (ie
postgres://user:password@host:post/database).

The results.json file contains json list containing the query, time to
run, results, and settings for that query.

Settings and explanations
==========================

TODO

Enabling Logging
=================

need to turn on Postgres logging in postgresql.conf

log_destination = stderr
logging_collector = on
log_filename = 'pgsql-%Y-%m-%d.log'
log_min_error_statement = 'error'
log_connections = false
log_line_prefix = '<%t>'
log_statement = all
log_directory = '/var/log/pglog'

* Make sure log_directory exists and has correct permissions
* Restart postgres

"""

# Copyright (c) 2010 Matt Harrison
from itertools import islice, repeat, izip, cycle
import json
import optparse
import re
import sys
import time

import psycopg2

import meta

STATEMENT = 'statement:'
SEP = '-----\n'

def process_dburl(dburl):
    """
    Convert sqlalchemy style urls to psycopg2 friendly params
    >>> process_dburl('postgres://matt:password@localhost:9999/testdb')
    ('localhost', 'testdb', 'matt', 'password', '9999')
    >>> process_dburl('postgres://matt/testdb')
    (None, 'testdb', 'matt', None, None)
    >>> process_dburl('postgres://matt')
    (None, '', 'matt', None, None)
    >>> process_dburl('postgres://matt:password')
    (None, '', 'matt', 'password', None)
    >>> process_dburl('postgres://matt@localhost')
    ('localhost', '', 'matt', None, None)
    >>> process_dburl('postgres://matt@localhost:9999')
    ('localhost', '', 'matt', None, '9999')
    """
    regex = re.compile(r'^postgres://(?P<name>\w+):?(?P<pass>\w+)?@?(?P<host>\w+)?:?(?P<port>\d\d\d\d)?/?(?P<db>\w*)')

    host = regex.search(dburl).group('host')
    dbname = regex.search(dburl).group('db')
    user = regex.search(dburl).group('name')
    password = regex.search(dburl).group('pass')
    port = regex.search(dburl).group('port')

    return host, dbname, user, password, port
        
def get_conn_string(dburl):
    """
    >>> get_conn_string('postgres://matt:password@localhost:9999/testdb')
    'dbname=testdb user=matt password=password host=localhost port=9999'
    """
    host, dbname, user, password, port = process_dburl(dburl)
    conn = []
    for param in ['dbname', 'user', 'password',
                  'host', 'port', 'sslmode']:
        value = locals().get(param, None)
        if value:
            conn.append('%s=%s' %(param, value))
    result = ' '.join(conn)
    return result
    
class Analyzer(object):
    def __init__(self, query, dburl=None):
        self.dburl = dburl
        self.query = query
        self.host = None
        self.dbname = None
        self.user = None
        self.password = None

    def _process_dburl(self):
        self.host, self.dbname, self.user, self.password, self.port = process_dburl(self.dburl)
        return
        regex = re.compile(r'^postgres://(?P<name>\w+):?(?P<pass>\w+)?@?(?P<host>\w+)?:?(?P<port>\d\d\d\d)?/(?P<db>\w*)')
        
        self.host = regex.search(self.dburl).group('host')
        self.dbname = regex.search(self.dburl).group('db')
        self.user = regex.search(self.dburl).group('name')
        self.password = regex.search(self.dburl).group('pass')
        self.port = regex.search(self.dburl).group('port')
    
    def run_config(self, config, fout):
        results = []
        for settings in config.gen_settings():
            setting_dict = dict(settings)
            with TweakContext(self.dburl, setting_dict) as tweaker:
                lines = list(x[0] for x in tweaker.run(add_explain_analyze(self.query)))
                results.append({'query':self.query,
                                'settings':setting_dict,
                                'time':tweaker.elapsed,
                                'results':lines})
        fout.write(json.dumps(results, indent=2))
                

    def analyze(self):
        conn = psycopg2.connect(get_conn_string(self.dburl))
        cur = conn.cursor()
        query = self.query
        for mod in ['ANALYZE', 'EXPLAIN']:
            if mod not in query:
                query = '%s %s' %(mod, query)

        cur.execute(query)
        results = [x[0] for x in cur.fetchall()]
        cur.close()
        conn.close()

def add_explain_analyze(query):
    for mod in ['ANALYZE', 'EXPLAIN']:
        if mod not in query:
            query = '%s %s' %(mod, query)
    return query

    
class Config(object):
    """
    A config is a bunch a setting names with possible values.
    When you call gen_settings it iterates over the combinations of those settings

    The config file is persisted as a json file
    
    >>> c = Config()
    >>> c.add_setting('foo', ['1', '100'])
    >>> c.add_setting('bar', ['2', '200'])
    >>> list(c.gen_settings())
    [[('foo', '1'), ('bar', '2')], [('foo', '1'), ('bar', '200')], [('foo', '100'), ('bar', '2')], [('foo', '100'), ('bar', '200')]]
    """
    def __init__(self):
        self.settings = []

    def add_setting(self, name, param_values):
        self.settings.append((name, param_values))

    def gen_settings(self):
        setting_groups = []
        for key, values in self.settings:
            setting_groups.append([(key,y) for y in values])
        for combo in iterCombinations(*setting_groups):
            yield combo

    def write(self, fout):
        fout.write(json.dumps(self.settings, indent=2))

def load_config(fin):
    c = Config()
    c.settings = json.loads(fin.read())
    return c

def iterCombinations(*iterables, **kwds):
    '''
    >>> #list(iterCombinations(*[[('a',1), ('a',2)],[('b', 1), ('b', 2)]]))
    >>> #list(iterCombinations([('a',1), ('a',2)],[('b', 1), ('b', 2)]))
    >>> list(iterCombinations(range(5), 'abc'))
    [[0, 'a'], [0, 'b'], [0, 'c'], [1, 'a'], [1, 'b'], [1, 'c'], [2, 'a'], [2, 'b'], [2, 'c'], [3, 'a'], [3, 'b'], [3, 'c'], [4, 'a'], [4, 'b'], [4, 'c']]
    http://code.activestate.com/recipes/501151-generating-combinations-in-blocks/
    Generates the combinations of the given iterables.
    
    @returns: An iterator over all combinations of the C{iterables}. Each yielded
        combination is a list of size C{len(iterables)} where the i-th element
        is drawn from the i-th iterable. B{Important note:} This list should not
        be modified between iterations; if you need to modify it, copy it to a
        new container.
    
    @param iterables: One or more arbitrary iterables.
    @param kwds: Currently only 'blocksize' allowed.
    @keyword blocksize: Determines the order of the yielded combinations. By
        default (C{blocksize=1}), the first iterable corresponds to the most outer
        loop (slowest change) and the last iterable corresponds to the most inner
        loop (fastest change).
        
        For larger blocksize, each iterable is first partitioned into consecutive
        blocks of size C{blocksize} (except perhaps for the last block which may
        be shorter). Then each combination is yielded by first iterating over
        each block combination C{B := (B1,B2,..Bn)} and then yielding each
        combination from B.
        
        More generally, C{blocksize} can be an iterable so that different
        C{iterables} can be partitioned by different block size. In this case,
        C{blocksize} is repeated as many time as necessary to match C{len(iterables)}.
        For instance::
            iterCombinations(range(4),range(6),range(8),'xyz', blocksize=(2,3))
        partitions C{range(4)} and C{range(8)} with blocksize=2, while C{range(6)}
        and 'xyz' are partitioned with blocksize=3.
    '''
    combo = [None] * len(iterables)
    blocksize = kwds.get('blocksize', 1)
    if isinstance(blocksize, int):
        sizes = repeat(blocksize)
    else:
        sizes = cycle(iter(blocksize))
    block_lists = [list(_iterblocks(it,sz)) for it,sz in izip(iterables,sizes)]
    for block_combo in _iterCombinations(block_lists, [None] * len(iterables)):
    #for block_combo in _iterCombinations(block_lists, len(iterables)):
        combo = [None] * len(iterables)
        for _ in _iterCombinations(block_combo, combo):
        #for _ in _iterCombinations(block_combo, len(iterables)):
            yield combo


def _iterCombinations(groups, combo_list, index=0):
    """
    >>> list(_iterCombinations('abc', [None, None, None]))
    [['a', 'b', 'c']]
    >>> #list(_iterCombinations('abc', 1))

    # note that combo_list persists over time... hence this problem
    >>> list(_iterCombinations(['abc', 'xyz'], [None]* 2))
    [['c', 'z'], ['c', 'z'], ['c', 'z'], ['c', 'z'], ['c', 'z'], ['c', 'z'], ['c', 'z'], ['c', 'z'], ['c', 'z']]
    
    """
    # generate recursively all combinations of groups, updating combo_list
    # *in-place* for each combination.
    if index < len(groups)-1:
        for x in groups[index]:
            combo_list[index] = x
            for foo in _iterCombinations(groups,combo_list,index+1):
                yield combo_list
    else: # optimization to avoid the last level of recursion
        assert index == len(groups)-1
        for x in groups[index]:
            combo_list[index] = x
            yield combo_list

        
def _iterblocks(iterable, blocksize, factory=tuple):
    """
    >>> list(_iterblocks('abc', 1))
    [('a',), ('b',), ('c',)]
    >>> list(_iterblocks('abc', 2))
    [('a', 'b'), ('c',)]
    >>> list(_iterblocks('abc', 4)) == list(_iterblocks('abc', 3))
    True
    """
    # split the iterable into blocks of blocksize
    iterable = iter(iterable)
    while True:
        block = factory(islice(iterable,blocksize))
        if not block: break
        yield block
        if len(block) < blocksize: break

def analyze_queries(query_lines, dburl, config_file, fout):
    config = load_config(config_file)
    for query in query_lines:
        a = Analyzer(query, dburl)
        a.run_config(config, fout)

def _get_qf_queries(fin, query_separator=SEP):
    """
    Yield lines of a single query
    """
    lines = []
    for line in fin:
        if line == query_separator:
            if lines:
                yield ''.join(lines)
            lines = []
        else:
            lines.append(line)
    if lines:
        yield ''.join(lines)
            

def get_log_queries(fin, fout, query_separator=SEP):
    for q_lines in _query_lines(fin):
        for line in q_lines:
            fout.write(line)
        fout.write(SEP)
            
def _query_lines(fin):
    """
    Given a query log yield lines of a single query
    """
    in_q = False
    lines = []
    for line in fin:
        if in_q and not line.startswith('\t'):
            in_q = False
            if lines:
                yield lines
                lines = []
        if not in_q and STATEMENT in line:
            in_q = True
            start_idx = line.index(STATEMENT) + len(STATEMENT)
            line = line[start_idx:]
        if in_q:
            lines.append(line)
    if lines:
        yield lines

def get_expected_real_row(lines):
    """
    >>> get_expected_real_row(['Seq Scan on promo_dim  (cost=0.00..1.51 rows=51 width=11) (actual time=0.004..0.052 rows=54 loops=1)', 'Total runtime: 0.150 ms'])
    (51, 54)
    """
    ACT = 'actual time='
    regex = re.compile(r'.*\(cost=.* rows=(?P<rows_est>\d*) .*\) \(actual.* rows=(?P<rows_real>\d*) .*\).*')
    for line in lines:
        if ACT in line:
            est = regex.search(line).group('rows_est')
            real = regex.search(line).group('rows_real')
            return int(est), int(real)
    return None
        
        
def get_runtime(lines):
    """
    >>> get_runtime(['Seq Scan on promo_dim  (cost=0.00..1.51 rows=51 width=11) (actual time=0.004..0.052 rows=51 loops=1)', 'Total runtime: 0.150 ms']) <= 0.15
    True
    """
    TOT = 'Total runtime: '
    for line in lines:
        if line.startswith(TOT):
            assert line.endswith(' ms')
            idx = line.index(TOT) + len(TOT)
            return float(line[idx:-3])
    return None

def run_sql(cur, sql):
    cur.execute(sql)
    try:
        for row in cur.fetchall():
            yield row
    except psycopg2.ProgrammingError, e:
        if 'no results to fetch in e':
            pass
        else:
            raise

class TweakContext(object):
    def __init__(self, dburl, settings_dict):
        self.dburl = dburl
        self.settings_dict = settings_dict
        self.conn = None
        self.cur = None
        self.old_settings = {}
        self.elapsed = None

    def __enter__(self):
        self.conn = psycopg2.connect(get_conn_string(self.dburl))
        self.cur = self.conn.cursor()
        # get old db settings
        for key in self.settings_dict:
            sql = 'SHOW %s' % key
            results = list(run_sql(self.cur, sql))
            self.old_settings[key] = results[0][0]
            
        # set database settings
        for key, value in self.settings_dict.items():
            sql = "SET %s = '%s'" %(key, value)
            results = list(run_sql(self.cur, sql))

        # need to return the as part
        return self
            
    def run(self, query):
        start = time.time()
        for row in run_sql(self.cur, query):
            yield row
        self.elapsed = time.time() - start

    def __exit__(self, *args):
        # set old db settings
        for key, value in self.old_settings.items():
            sql = "SET %s = '%s'" %(key, value)
            results = list(run_sql(self.cur, sql))
        
        self.cur.close()
        self.conn.close()
    
    

def main(prog_args):
    parser = optparse.OptionParser(version=meta.__version__)
    extract_group = optparse.OptionGroup(parser, 'Extract Queries from psql log')
    extract_group.add_option('--extract-queries', action='store_true', help='extract queries from a log file')
    extract_group.add_option('--log-file', help='specify log file')
    parser.add_option_group(extract_group)

    analyze_group = optparse.OptionGroup(parser, 'Analyze extracted queries')
    analyze_group.add_option('--analyze-queries', action='store_true', help='analyze query file (or stdin)')
    analyze_group.add_option('--query-file', help='specify query file (file with queries delimited by %s or stdin)' % SEP)
    analyze_group.add_option('--dburl', help='sqlalchemy connection string (we do not use sa, just the style of it ie postgres://postgres:password@localhost:port/dbname)')
    analyze_group.add_option('--config-file', help='specify config file containing parameters for analyzing')
    parser.add_option_group(analyze_group)

    parser.add_option('--gen-config', action='store_true', help='generate a config file writeing work_mem and maintenance_work_mem')
    
    parser.add_option('--test', action='store_true', help='run doctests')
    
    
    opt, args = parser.parse_args(prog_args)

    if opt.extract_queries:
        if not opt.log_file:
            sys.error('Specify --log-file')
            return 1
        fin = open(opt.log_file)
        fout = sys.stdout
        get_log_queries(fin, fout)
        fout.close()

    if opt.analyze_queries:
        if not opt.query_file:
            fin = sys.stdin
        else:
            fin = open(opt.query_file)
        if not opt.config_file:
            sys.error('Specify --config-file (--gen-config to generate sample)')
        
        analyze_queries(_get_qf_queries(fin), opt.dburl, open(opt.config_file), sys.stdout)

    if opt.gen_config:
        c = Config()
        c.add_setting('work_mem', ['1MB', '512MB'])
        c.add_setting('maintenance_work_mem', ['1MB', '512MB'])
        c.write(sys.stdout)
        
    if opt.test:
        import doctest
        doctest.testmod()

if __name__ == '__main__':
    sys.exit(main(sys.argv))

